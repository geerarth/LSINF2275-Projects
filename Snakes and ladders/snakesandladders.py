# -*- coding: utf-8 -*-
"""Snakes&Ladders

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oiT5p_eRhJs6Bwya5FuIAx3qRUJBcYNf

SnakesAndLadders, Simulation, markovDecision
"""

import random
import numpy as np
import os, time

class SnakesAndLadders:

    DEBUG = False; INFO = False

    DICE_SAFE = {"index": 1, "name": "SAFE", "values": [0,1], "immune": 1.0}
    DICE_NORMAL = {"index": 2, "name": "NORMAL", "values": [0,1,2], "immune": 0.5}
    DICE_RISKY = {"index": 3, "name": "RISKY", "values": [0,1,2,3], "immune": 0.0}

    NOT_TRAP, TRAP_RESTART, TRAP_PENALTY, TRAP_PRISON, TRAP_GAMBLE = range(0, 5)

    # Board ladders (indexes start at 0)
    BOARD = {
        2: [3,10],
        9: 14
    }

    # ==============

    def __init__(self, layout: np.ndarray = np.zeros(15), circle: bool = False):
        assert layout[0] == self.NOT_TRAP and layout[-1] == self.NOT_TRAP, f"The first and final squares cannot be trapped"

        self.layout = layout
        self.circle = circle
        self.length = len(layout)-1

        self.setActions([self.DICE_SAFE, self.DICE_NORMAL, self.DICE_RISKY])

    def isEnd(self, state: int): return state == self.length        

    def getStates(self): return range(len(self.layout))
    def getActions(self): return self.actions

    def setActions(self, actions: list):
        assert len(actions) > 0, f"The list of actions cannot be empty."
        self.actions = actions

    # ==============

    # This corresponds to a multidimensional matrix of size (15, 3)
    # With values formatted as tuple : (nextState, probability, cost)
    def customTransitionMatrix(self):
        matrix = np.zeros((len(self.layout), len(self.actions)), dtype=object)

        for state in self.getStates():
            for action in self.getActions():
                matrix[state, action.get('index')-1] = self.nextProbCost(state, action)

        return matrix


    # Compute the cost, the probability of all possible (future) states
    # for a given state and a given action
    def nextProbCost(self, state: int, action: dict):

        assert state >= 0 and state <= self.length, f"State cannot be outside of the board"
        assert action in self.actions, f"{action.get('name')} is not in the list of possible actions."

        returns = []

        immune = action.get('immune')
        values = action.get('values') # dice values
        probs = 1/len(values) # ex: [0,1,2] => prob is 1/3 of each value

        # For each possible value rolled by the dice
        for steps in values: 
            current = state

            # If we don't move, we only check for trap
            if steps == 0:
                returns += self.activateTrap(current, probs, immune)
            else: 
                # Otherwise, we do a first step. See forwardStep !
                # straight=False means that we can take an intersection
                possible = self.forwardStep(state, 1, False)
                for path in possible:
                    # For every path possible, we go forward (straight=True) by the number of steps left
                    nextState, = self.forwardStep(path, steps-1, True)
                    nextState = self.dontOverflow(nextState) # Handling the board end and "circle" parameter
                    returns += self.activateTrap(nextState, probs/len(possible), immune)
                    # the probability is now P(dice value & taking the path)
                    
        if (self.DEBUG): print("RETURNS for state", state, "and action", action.get('name'), "are", returns)
        return returns 

    def activateTrap(self, state: int, probs: float, immune: float):
        # assert state >= 0 and state <= self.length, f"State cannot be outside of the board"
        state = self.dontOverflow(state)

        square = self.layout[state] # return the int value of the layout
        inactive, active = probs * immune, probs * (1-immune) # conditional probabilities

        if square == self.NOT_TRAP or immune == 1:
            return [(state, probs, 1)]

        elif square == self.TRAP_PRISON:
            return [(state, active, 2), (state, inactive, 1)] # cost is 2 if prison (as we wait 1 turn)

        elif square == self.TRAP_RESTART:
            return [(0, active, 1), (state, inactive, 1)]

        elif square == self.TRAP_GAMBLE:
            # We compute the probability of being teleported to any case
            # That's 1/nb_of_case multiplied by the prob of activation
            tp_probs = 1/len(self.layout)
            returns = [(nextState, active*tp_probs, 1) for nextState in self.getStates()]
            return returns + [(state, inactive, 1)]

        elif square == self.TRAP_PENALTY:
            nextState = self.reverseStep(state, 3) # we step back by 3
            return [(nextState, active, 1), (state, inactive, 1)]

        else: 
            if self.DEBUG: print("UNEXPECTED TRAP: ", square)
            return [(state, 0, -1)]

    # ==============

    # Recursive
    # Returns possible state after x steps forward as a list
    # If straight is false, we check the board and can take an intersection
    def forwardStep(self, state, steps=1, straight=True):

        if steps <= 0: 
            if type(state) is list: return state
            else: return [state]

        split = self.BOARD.get(state) # check if thats a special case
        if type(split) is int: nextState = split # if we jump
        elif type(split) is list and not straight: nextState = split # if thats a split
        else: nextState = state+1 # otherwise just step 1

        return self.forwardStep(nextState, steps-1, True)

    # Recursive
    # Same as forwardStep but we go in reverse
    # Does not return the state as a list (inconsistency but we can only be on a single square)
    def reverseStep(self, state, steps=1):
        if steps <= 0:  return state
        # Match the intersections (self.BOARD) but backward
        # basically we find key by value of a dict
        nextState = state - 1
        for k,v in self.BOARD.items():
            if v == state: nextState = k
            if type(v) == list: 
                if state in v: nextState =  k

        return self.reverseStep(max(nextState, 0), steps-1)

    def dontOverflow(self, state: int):
        if state < 0: state = 0
        if state > self.length:
            if self.circle: state -= self.length # if circle, get back to start
            else: state = self.length # otherwise, we're at the end
        return state

class Simulation(SnakesAndLadders):

    def __init__(self, layout: np.ndarray, circle: bool, strategy: np.ndarray):
        assert len(strategy) == len(layout)-1, f"The given strategy size is incorrect. Must be {len(layout)-1}."
        super().__init__(layout, circle)
        self.strategy = strategy
        self.reset()

    def setStrategy(self, strategy: np.ndarray): self.strategy = strategy

    def reset(self):
        self.state = 0
        self.turns = 0

    # Retrieve the dice dictionary from the strategy value
    def getDice(self, current_strategy: int):
        for action in self.getActions():
            if action.get("index", -1) == current_strategy:
                return action
        return None

    def rollDice(self):

        current_strategy = self.strategy[self.state]
        current_dice = self.getDice(current_strategy)

        assert current_dice is not None, f"The strategy {current_strategy} doesn't match any available action."

        immune = current_dice.get("immune")
        obtained_value = random.choice(current_dice.get("values"))

        next_state = self.state

        if obtained_value > 0:
            possible = self.forwardStep(next_state, 1, False) # forwardStep always returns a list, even if 1 possible element
            next_state = random.choice(possible) # so we always use random.choice without problem
            next_state, = self.forwardStep(next_state, obtained_value-1, True)
            next_state = self.dontOverflow(next_state)

        next_state, turns = self.activateTrap(next_state, immune) # check for trap on the case we ended

        self.state = next_state
        self.turns += turns

    # Redefine the function as it need to return the next state and not the probability
    def activateTrap(self, state: int, immune: float):
        #assert state >= 0 and state <= self.length, f"State cannot be outside of the board"
        state = self.dontOverflow(state)

        square = self.layout[state] 
        activate = random.random() > immune

        if not activate: return (state, 1)
        else: 
            if square == self.NOT_TRAP or immune == 1: return (state, 1)
            elif square == self.TRAP_PRISON: return (state, 2)
            elif square == self.TRAP_RESTART: return (0, 1)
            elif square == self.TRAP_GAMBLE: return (random.choice(self.getStates()), 1)
            elif square == self.TRAP_PENALTY:
                nextState = self.reverseStep(state, 3)
                return (nextState, 1)
            else: return (state, 1)

    # Returns the number of turn needed in order to end a simulated game
    def play(self):
        self.reset()
        while not self.isEnd(self.state): self.rollDice()
        return self.turns
    
    # Returns the mean of x played game
    def empiricalTurns(self, number_of_simulations: int = 500_000) :
        return np.mean([self.play() for _ in range(number_of_simulations)]) 


    # Returns [SAFE, NORMAl, RISKY, RANDOM] empirical turns
    def computeSubOptimalStrategies(self, number_of_simulations: int = 100_000, random_M: int = 1000, random_m: int = 1000):
        oldStrat = self.strategy
        returns = []

        # suboptimal strategy 1, 2 and 3 (one dice strategies)
        for i in range(1,4):
            sub = [i] * len(self.strategy)
            self.setStrategy(sub)
            returns.append(self.empiricalTurns(number_of_simulations))

        # suboptimal strategy 4 (random strategy)
        randoms = np.zeros(random_M)
        for i in range(random_M) :
            randStrat = np.random.randint(low = 1, high = 4, size = 14)
            self.setStrategy(randStrat)
            randoms[i] = self.empiricalTurns(random_m)

        returns.append(np.mean(randoms))
        self.setStrategy(oldStrat)
        return returns


""" Requested method """
def markovDecision(layout: np.ndarray, circle: bool = False):
    # Instanciate the game
    sal = SnakesAndLadders(layout, circle)
    
    # Set all values to 1 by default
    V = np.ones(len(layout))
    V[sal.length] = 0 # except for the end

    # Define a threshold and some booleans for printing
    threshold = 1e-6; max_iterations = 1000
    done, failed, iteration = False, False, 0
    
    # ==============

    # Compute our "transition matrix" once
    next_prob_cost_matrix = sal.customTransitionMatrix()
    if (sal.DEBUG): print(next_prob_cost_matrix)

    # Function to returns the updated value for a state and an action
    def Q(state: int, action: dict):
        # Formula : see slide 121 (p61) 
        index = action.get("index")-1

        return sum(prob * (cost + V[nextState]) \
                   for nextState, prob, cost in next_prob_cost_matrix[state, index])

    # ==============
    while not done: 
        iteration+=1

        v = np.copy(V)
        policy = {}

        # For all possible states
        for state in sal.getStates():

            # Except for the end state
            if not sal.isEnd(state): 
                # Update the value for every combinaison of state and action
                actions = sal.getActions()
                Qval = [Q(state, action) for action in actions]

                # Retrieve the one that _minimize_ the value (as we have a _positive_ cost)
                V[state] = min(Qval)
                policy[state] = actions[np.argmin(Qval)]
                if sal.DEBUG: print("It:", iteration, "||", V[state], policy[state].get("name"))
            else: 
                V[state] = 0
                policy[state] = {"name": "END"}    

        # Check if the values converge
        delta = max(abs(V[state] - v[state]) for state in sal.getStates())
        if delta <= threshold: done = True
        if iteration >= max_iterations: 
            done = True
            failed = True           

    # ==============

    policy_int = np.array([p.get("index") for p in policy.values()])
    best_strategy = policy_int[:sal.length]
    average_turns = V[:sal.length]

    # Printing (for debug purposes)

    if sal.INFO:
        if done:  
            print("\033[1m{:^15} {:^15} {:^15}\033[0m".format("Case", "V(s)", "Best"))
            for state in sal.getStates():
                print("{:^15} {:^15.3} {:^15}".format(state+1, V[state], policy[state].get("name")))
            print("")

        if failed : print("\033[91m", "[WARNING] The algorithm was unable to converge in", max_iterations, "iterations and a threshold of", threshold, "..", "\033[0m")
        else: print("\033[92m", "[INFO] Convergence achieved with a threshold of", threshold, "in", iteration, "iterations.", "\033[0m")

        print("\033[92m", "[INFO] The layout provided was:", layout, "\033[0m")
        print("")
        print("\033[93m", "[INFO] Optimal solution found:", best_strategy,  "\033[0m")
        print("\033[93m", "[INFO] Average number of turns:", average_turns,  "\033[0m")


    # Return the results
    return [average_turns, best_strategy]